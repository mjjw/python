{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP)\n",
    "===\n",
    "(1) NLP Tasks: How does it perform? \n",
    "    \n",
    "    (a) Well: Spam detection, Parts-of-Speech (POS) tagging see www.parts-of-speech.info, and Named-Entity-Recognition (NER).\n",
    "    \n",
    "    (b) Good: Sentiment analysis, Machine Translation see https://translate.google.com, and Information extraction\n",
    "    \n",
    "    (c) Sometimes: Machine conversations (recognize speech, wreck a nice beach), Paraphrasing and summarization.  e.x., pinterest.\n",
    "    \n",
    "    (d) Interesting progress: word2vec (uses ann)\n",
    "\n",
    "(2) Spam Detector\n",
    "\n",
    "(3) Sentiment Analyzer\n",
    "\n",
    "(4) Exlpore NLTK\n",
    "\n",
    "(5) Latent Semantic Analysis (LSA)\n",
    "\n",
    "(6) Article spinner\n",
    "\n",
    "Why is NLP Hard? Math is universal, but language is ambiguous! For example, \"Republicans Grill IRS Chief Over Lost Emails\" could be interpretted as \"Republicans harshly question the chief about emails\" or \"Republicans cook the chief using email as fuel\". Another example, \"I saw a man on a hill with a telescope.\" could be interpretted as \"There's a man on a hill and I'm watching him with a telescope.\" , \"There's a man on a hill, who I'm seeing, and he has a telescope.\", \"There's a man, and he's on a hill that also has a telescope on it.\", \"I'm on a hill, and I saw a man using a telescope.\"  Non-formal language: Twitter is limited to 140 characters, \"U\", \"UR\", \"LOL\", \"netflix and chill\"  Remember language = text and voice. Voice requires signal processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam Detector\n",
    "---\n",
    "Let's get the pre-processed data here:\n",
    "http://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "2 main takeaways:\n",
    "(1) A lot of NLP is just pre-processing data, so we can use ML algorithms we already know.\n",
    "\n",
    "(2) You can choose ANY ML algorithm as long as you can make the data fit.\n",
    "\n",
    "Sci-kit learn.\n",
    "\n",
    "Pre-processing \n",
    "---\n",
    "Columns 1...48\n",
    "\n",
    "word frequncy measure - number of times word appears divided by the number of words in the documents x 100\n",
    "\n",
    "Last column is a label\n",
    "\n",
    "1=spam, 0=not spam\n",
    "One example of a \"term-document matrix\" - terms go along columns, documents (aka emails) go along rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for NB: 0.85\n"
     ]
    }
   ],
   "source": [
    "# sklearn.naive_bayes.MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"/home/mike/Downloads/spambase/spambase.data\").as_matrix()\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:,:48]\n",
    "Y = data[:,-1]\n",
    "\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(Xtrain,Ytrain)\n",
    "print \"Classification rate for NB:\", model.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for AdaBoost: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(Xtrain,Ytrain)\n",
    "print \"Classification rate for AdaBoost:\", model.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Types of Features\n",
    "---\n",
    "\n",
    "Most of these fit into what we call \"bag of words\")\n",
    "\n",
    "(1) word proportion (we already saw this)\n",
    "\n",
    "(2) raw word counts\n",
    "\n",
    "(3) binary (1 if word appears, o otherwise)\n",
    "\n",
    "(4) TF-IDF (takes into account the fact that some words appear in many documents, and hence don't really tell us much)\n",
    "    \n",
    "sklearn: http://scikit-learn.org/stable/modules/feature_extractions.html\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a very simple Sentiment Analyzer\n",
    "---\n",
    "What is it? \n",
    "    \n",
    "    sentiment = how positive or negative some text is\n",
    "    \n",
    "    Amazon reviews, Yelp reviews, hotel reviews, tweets, ...\n",
    "    \n",
    "Our data:\n",
    "https://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "\n",
    "Outline of our Sentiment Analyzer\n",
    "---\n",
    "We'll just look at the electronics category, but you can try the same code on others.  We could use 5 star targets to do regression, but let's just do classification since they are already marked \"positive\" and \"negative\".  We'll need an XML parser and it will be BeautifulSoup.  We're only going to look at the key \"review_text\" and ignore the extra data. To create our feature vector, we're going to do the same thing we did in the 1st dataset were we counted up the number of occurences of each word and divide by the total number of words.  For that to work we'll need 2 passes, one to collect the total number of distinct words so we know the size of our feature vector and possibly remove stop words like this, is, or I to reduce the vocabulary size.  This is so we'll know the index of each token/word.  On the second pass, we'll be able to assign the values to the data vector.['/ After that, we can just use any SKLearn Classifier as we did previously, but we'll use logistic regression so we can interpret the weights of the learned model to get a score for each individual input word.  That will tell us if you see a word like horrible with a weight of -1 is associated with negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate: 0.67\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# this WordNetLemmatizer turns words into their base form such as dogs and dog are the same word.  \n",
    "# So we don't want our vocabulary size to be too large. \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "stopwords = set(w.rstrip() for w in open('/home/mike/Downloads/machine_learning_examples-master/nlp_class/stopwords.txt'))\n",
    "\n",
    "# load the reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('/home/mike/Downloads/machine_learning_examples-master/nlp_class/electronics/positive.review').read())\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('/home/mike/Downloads/machine_learning_examples-master/nlp_class/electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')\n",
    "\n",
    "# there are more positive reviews than negative reviews\n",
    "# so let's take a random sample so we have balanced classes\n",
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]\n",
    "\n",
    "# first let's just try to tokenize the text using nltk's tokenizer\n",
    "# let's take the first review for example:\n",
    "# t = positive_reviews[0]\n",
    "# nltk.tokenize.word_tokenize(t.text)\n",
    "#\n",
    "# notice how it doesn't downcase, so It != it\n",
    "# not only that, but do we really want to include the word \"it\" anyway?\n",
    "# you can imagine it wouldn't be any more common in a positive review than a negative review\n",
    "# so it might only add noise to our model.\n",
    "# so let's create a function that does all this pre-processing for us\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# create a word-to-index map so that we can create our word-frequency vectors later\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "\n",
    "# now let's create our input matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # normalize it before setting label\n",
    "    x[-1] = label\n",
    "    return x\n",
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "# shuffle the data and create train/test splits\n",
    "# try it multiple times!\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "# last 100 rows will be test\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print \"Classification rate:\", model.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit -0.717914355533\n",
      "fit 0.538816456084\n",
      "easy 1.76958660878\n",
      "support -0.86059544655\n",
      "happy 0.690601251567\n",
      "time -0.546546903041\n",
      "love 1.21653505251\n",
      "returned -0.791150191408\n",
      "cable 0.645920811086\n",
      "company -0.514905050724\n",
      "paper 0.560488992688\n",
      "try -0.659793813561\n",
      "customer -0.686208736099\n",
      "perfect 0.976268261198\n",
      "waste -0.994578948086\n",
      "highly 1.03353133764\n",
      "then -1.08902468091\n",
      "wa -1.56369137092\n",
      "space 0.626725466711\n",
      "price 2.74307767646\n",
      "using 0.660881455752\n",
      "lot 0.680385338126\n",
      "you 1.05070795654\n",
      "poor -0.759043961336\n",
      "month -0.806202631089\n",
      "tried -0.737323838327\n",
      "stopped -0.551823183193\n",
      "pretty 0.760439720798\n",
      "look 0.547474831485\n",
      "quality 1.3335500341\n",
      "speaker 0.954018470374\n",
      "ha 0.743957343761\n",
      "recommend 0.643257957902\n",
      "doe -1.18245881385\n",
      "bad -0.690957509065\n",
      "mouse 0.532031083526\n",
      "item -0.971515536974\n",
      "little 0.901291355375\n",
      "sound 1.09392919589\n",
      "n't -2.11720680364\n",
      "money -1.04915938476\n",
      "'ve 0.817587331682\n",
      "hour -0.553014507251\n",
      "bit 0.647221141343\n",
      "comfortable 0.639402392728\n",
      "value 0.525073779817\n",
      "buy -0.885609434453\n",
      "excellent 1.32973019364\n",
      "expected 0.583999000268\n",
      "laptop 0.519391607481\n",
      "picture 0.575763103212\n",
      "memory 0.965142496283\n",
      "refund -0.629141190723\n",
      "week -0.673362021552\n",
      "return -1.21046968338\n",
      "fast 0.904478349023\n",
      "warranty -0.543534403926\n",
      "junk -0.534688537209\n"
     ]
    }
   ],
   "source": [
    "# let's look at the weights for each word\n",
    "# try it with different threshold values!\n",
    "threshold = 0.5\n",
    "for word, index in word_index_map.iteritems():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print word, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
